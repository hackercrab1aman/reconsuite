import requests
import logging
from urllib.parse import urljoin, urlparse, parse_qs
from bs4 import BeautifulSoup
import re
from .utils import rate_limit, sanitize_url

class VulnerabilityScanner:
    """Module for basic vulnerability scanning"""
    
    def __init__(self, target, max_pages=10, timeout=5):
        self.target = sanitize_url(target)
        self.logger = logging.getLogger(__name__)
        self.max_pages = max_pages
        self.timeout = timeout
        self.scanned_urls = set()
        
        # User agent to mimic a browser
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # Define payloads for testing
        self.xss_payloads = ["'", "\"", "<script>alert(1)</script>", "<img src=x onerror=alert(1)>"]
        self.sqli_payloads = ["'", "\"", "1' OR '1'='1", "1\" OR \"1\"=\"1"]
        self.lfi_payloads = ["../../../etc/passwd", "../../../../etc/passwd", "../../../../../etc/passwd"]
    
    def _extract_links(self, url, html):
        """Extract links from HTML content"""
        parsed_url = urlparse(url)
        base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
        
        soup = BeautifulSoup(html, 'html.parser')
        links = []
        
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            if href.startswith('/') or href.startswith('./'):
                # Relative URL
                links.append(urljoin(base_url, href))
            elif href.startswith('http') and parsed_url.netloc in href:
                # Absolute URL but same domain
                links.append(href)
        
        return links
    
    def _extract_forms(self, url, html):
        """Extract forms from HTML content"""
        soup = BeautifulSoup(html, 'html.parser')
        forms = []
        
        for form in soup.find_all('form'):
            action = form.get('action', '')
            method = form.get('method', 'get').lower()
            inputs = []
            
            for input_tag in form.find_all(['input', 'textarea']):
                input_type = input_tag.get('type', '')
                input_name = input_tag.get('name', '')
                
                if input_name and input_type != 'submit':
                    inputs.append(input_name)
            
            form_url = urljoin(url, action) if action else url
            forms.append({
                'action': form_url,
                'method': method,
                'inputs': inputs
            })
        
        return forms
    
    @rate_limit(1)  # 1 second delay between requests
    def crawl_page(self, url):
        """Crawl a page and extract links and forms"""
        if url in self.scanned_urls or len(self.scanned_urls) >= self.max_pages:
            return None
        
        self.scanned_urls.add(url)
        
        try:
            response = requests.get(url, headers=self.headers, timeout=self.timeout)
            if response.status_code == 200 and 'text/html' in response.headers.get('Content-Type', ''):
                links = self._extract_links(url, response.text)
                forms = self._extract_forms(url, response.text)
                
                return {
                    'url': url,
                    'links': links,
                    'forms': forms,
                    'content': response.text
                }
            
        except requests.RequestException as e:
            self.logger.debug(f"Error crawling {url}: {str(e)}")
        
        return None
    
    def _test_xss(self, forms):
        """Test for XSS vulnerabilities in forms"""
        vulnerabilities = []
        
        for form in forms:
            for input_name in form['inputs']:
                for payload in self.xss_payloads:
                    try:
                        data = {name: 'test' for name in form['inputs']}
                        data[input_name] = payload
                        
                        if form['method'] == 'post':
                            response = requests.post(form['action'], data=data, headers=self.headers, timeout=self.timeout)
                        else:
                            response = requests.get(form['action'], params=data, headers=self.headers, timeout=self.timeout)
                        
                        # Check if the payload is reflected in the response
                        if payload in response.text and not payload.startswith("'") and not payload.startswith('"'):
                            vulnerabilities.append({
                                'type': 'XSS',
                                'url': form['action'],
                                'method': form['method'],
                                'param': input_name,
                                'payload': payload,
                                'description': f"Possible XSS vulnerability found in {input_name} parameter"
                            })
                            break  # Found vulnerability in this input, move to next
                            
                    except requests.RequestException:
                        continue
        
        return vulnerabilities
    
    def _test_sqli(self, forms):
        """Test for SQL injection vulnerabilities in forms"""
        vulnerabilities = []
        error_patterns = [
            "SQL syntax", 
            "mysql_fetch_array", 
            "ORA-", 
            "PostgreSQL",
            "SQLite3::",
            "you have an error in your sql syntax",
            "warning: mysql_",
            "unclosed quotation mark",
            "syntax error at line"
        ]
        
        for form in forms:
            for input_name in form['inputs']:
                for payload in self.sqli_payloads:
                    try:
                        data = {name: 'test' for name in form['inputs']}
                        data[input_name] = payload
                        
                        if form['method'] == 'post':
                            response = requests.post(form['action'], data=data, headers=self.headers, timeout=self.timeout)
                        else:
                            response = requests.get(form['action'], params=data, headers=self.headers, timeout=self.timeout)
                        
                        # Check for SQL error messages
                        for pattern in error_patterns:
                            if re.search(pattern, response.text, re.IGNORECASE):
                                vulnerabilities.append({
                                    'type': 'SQL Injection',
                                    'url': form['action'],
                                    'method': form['method'],
                                    'param': input_name,
                                    'payload': payload,
                                    'description': f"Possible SQL injection vulnerability found in {input_name} parameter"
                                })
                                break
                                
                    except requests.RequestException:
                        continue
        
        return vulnerabilities
    
    def _test_lfi(self, url_params):
        """Test for Local File Inclusion vulnerabilities in URL parameters"""
        vulnerabilities = []
        indicators = ["root:", "daemon:", "bin:", "sys:", "passwd:", "shadow:"]
        
        for url, params in url_params:
            for param_name in params:
                for payload in self.lfi_payloads:
                    try:
                        test_params = params.copy()
                        test_params[param_name] = payload
                        
                        response = requests.get(url, params=test_params, headers=self.headers, timeout=self.timeout)
                        
                        # Check for file content indicators
                        for indicator in indicators:
                            if indicator in response.text:
                                vulnerabilities.append({
                                    'type': 'LFI',
                                    'url': url,
                                    'method': 'get',
                                    'param': param_name,
                                    'payload': payload,
                                    'description': f"Possible Local File Inclusion vulnerability found in {param_name} parameter"
                                })
                                break
                                
                    except requests.RequestException:
                        continue
        
        return vulnerabilities
    
    def _extract_url_parameters(self, links):
        """Extract URL parameters from links"""
        url_params = []
        
        for link in links:
            parsed = urlparse(link)
            if parsed.query:
                params = parse_qs(parsed.query)
                if params:
                    # Convert from lists to single values for testing
                    single_params = {k: v[0] for k, v in params.items()}
                    url_without_query = link.split('?')[0]
                    url_params.append((url_without_query, single_params))
        
        return url_params
    
    def run(self):
        """Run vulnerability scan on target"""
        self.logger.info(f"Starting vulnerability scan for {self.target}")
        
        # Crawl pages
        all_forms = []
        all_links = []
        pages_to_crawl = [self.target]
        
        while pages_to_crawl and len(self.scanned_urls) < self.max_pages:
            url = pages_to_crawl.pop(0)
            page_data = self.crawl_page(url)
            
            if page_data:
                all_forms.extend(page_data['forms'])
                all_links.extend(page_data['links'])
                
                # Add new links to crawl
                for link in page_data['links']:
                    if link not in self.scanned_urls and link not in pages_to_crawl:
                        pages_to_crawl.append(link)
        
        self.logger.info(f"Crawled {len(self.scanned_urls)} pages, found {len(all_forms)} forms")
        
        # Extract URL parameters for LFI testing
        url_params = self._extract_url_parameters(all_links)
        
        # Run vulnerability tests
        xss_vulnerabilities = self._test_xss(all_forms)
        sqli_vulnerabilities = self._test_sqli(all_forms)
        lfi_vulnerabilities = self._test_lfi(url_params)
        
        # Combine results
        all_vulnerabilities = xss_vulnerabilities + sqli_vulnerabilities + lfi_vulnerabilities
        
        self.logger.info(f"Vulnerability scan completed. Found {len(all_vulnerabilities)} potential vulnerabilities")
        
        return {
            'target': self.target,
            'pages_scanned': len(self.scanned_urls),
            'forms_analyzed': len(all_forms),
            'total_vulnerabilities': len(all_vulnerabilities),
            'vulnerabilities': all_vulnerabilities
        }
